from aiogram import Bot, Dispatcher, types
from aiogram.utils import executor
from core.config import TOKEN
from core.pdf_tool import extract_text
from core.analyzer import analyze
from core.doc_generator import build
from core.kb_search import find_answer # –û—Å—Ç–∞–≤–ª—è–µ–º, –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –¥—Ä—É–≥–∏—Ö —Ü–µ–ª–µ–π
import pickle
import tempfile
import os
# --- –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π ---
from sentence_transformers import SentenceTransformer
import faiss
# --- –ö–æ–Ω–µ—Ü –∏–º–ø–æ—Ä—Ç–æ–≤ ---

# --- –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π ---
logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ NLP...")
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ FAISS –∏–Ω–¥–µ–∫—Å–∞...")
index = faiss.read_index("faiss_index.bin")

logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö...")
with open("metadata.pkl", "rb") as f:
    metadata_list = pickle.load(f)

logger.info("–ú–æ–¥–µ–ª—å, –∏–Ω–¥–µ–∫—Å –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã.")

# --- –§—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π ---
def search_in_knowledge_base(query_text, top_k=1, threshold=0.5):
    """–ü–æ–∏—Å–∫ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π."""
    if not query_text.strip():
        return []
    query_embedding = model.encode([query_text])
    faiss.normalize_L2(query_embedding.astype('float32'))
    scores, indices = index.search(query_embedding.astype('float32'), top_k)

    results = []
    for i in range(len(indices[0])):
        idx = indices[0][i]
        score = scores[0][i]
        if 0 <= idx < len(metadata_list) and score >= threshold: # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥—Ä–∞–Ω–∏—Ü –∏ –ø–æ—Ä–æ–≥–∞
            results.append({
                "metadata": metadata_list[idx],
                "score": float(score) # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ float –¥–ª—è JSON-—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            })
    return results
# --- –ö–æ–Ω–µ—Ü —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ–∏—Å–∫–∞ ---

API_TOKEN = TOKEN
bot = Bot(token=API_TOKEN)
dp = Dispatcher(bot)

user_data = {}   # {user_id: dict}

@dp.message_handler(commands="start")
async def start(m: types.Message):
    kb = types.ReplyKeyboardMarkup(resize_keyboard=True)
    kb.add("üìÑ –ê–Ω–∞–ª–∏–∑ PDF", "üìù –°–æ–∑–¥–∞—Ç—å –¥–æ–≥–æ–≤–æ—Ä", "‚ùì –ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å")
    await m.answer("üëã LegalAideIPbot ‚Äì –ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è –ò–ü", reply_markup=kb)

# –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–ª—è PDF
@dp.message_handler(content_types=types.ContentType.DOCUMENT)
async def handle_doc(m: types.Message):
    if not m.document.file_name.lower().endswith(".pdf"):
        return await m.reply("–ü—Ä–∏—à–ª–∏—Ç–µ PDF-–¥–æ–≥–æ–≤–æ—Ä")
    with tempfile.NamedTemporaryFile(suffix=".pdf", delete=False) as tmp:
        await m.document.download(tmp.name)
        text = extract_text(tmp.name)
        os.unlink(tmp.name)
        risks = "\n".join(analyze(text))
        await m.answer(risks)

# –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–ª—è –∫–Ω–æ–ø–∫–∏ "–°–æ–∑–¥–∞—Ç—å –¥–æ–≥–æ–≤–æ—Ä"
@dp.message_handler(lambda m: m.text == "üìù –°–æ–∑–¥–∞—Ç—å –¥–æ–≥–æ–≤–æ—Ä")
async def new_contract(m: types.Message):
    user_data[m.from_user.id] = {}
    await m.answer("–í–≤–µ–¥–∏—Ç–µ –≥–æ—Ä–æ–¥ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ú–æ—Å–∫–≤–∞):")

# –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–ª—è –∫–Ω–æ–ø–∫–∏ "–ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å"
@dp.message_handler(lambda m: m.text == "‚ùì –ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å")
async def ask(m: types.Message):
    await m.answer("–ù–∞–ø–∏—à–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å:")

# –û–±—â–∏–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–æ–æ–±—â–µ–Ω–∏–π
@dp.message_handler()
async def collect(m: types.Message):
    uid = m.from_user.id
    text = m.text.strip()
    data = user_data.setdefault(uid, {})

    # --- –ù–û–í–û–ï: –ü–æ–∏—Å–∫ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π ---
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–æ–≥–æ–≤–æ—Ä–∞
    # –ï—Å–ª–∏ –∫–ª—é—á–∏ "city", "landlord", "tenant", "property", "rent" –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ data,
    # –∑–Ω–∞—á–∏—Ç, –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Å–æ–∑–¥–∞–Ω–∏—è –¥–æ–≥–æ–≤–æ—Ä–∞.
    # –¢–∞–∫–∂–µ –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É, —á—Ç–æ —Ç–µ–∫—Å—Ç –Ω–µ –ø—É—Å—Ç–æ–π –∏ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –∫–æ–º–∞–Ω–¥–æ–π.
    if not data: # –ï—Å–ª–∏ —Å–ª–æ–≤–∞—Ä—å –ø—É—Å—Ç, –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–µ –Ω–∞—á–∞–ª —Å–æ–∑–¥–∞–Ω–∏–µ –¥–æ–≥–æ–≤–æ—Ä–∞
        kb_results = search_in_knowledge_base(text, top_k=1, threshold=0.6) # –ü–æ—Ä–æ–≥ –º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å

        if kb_results:
            best_match = kb_results[0]
            source_file = best_match['metadata'].get('source_file', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç')
            chunk_text = best_match['metadata'].get('original_chunk', '')[:500] + "..." # –û–±—Ä–µ–∑–∞–µ–º –¥–ª—è –≤—ã–≤–æ–¥–∞
            score = best_match['score']
            response_text = f"–ù–∞–π–¥–µ–Ω–æ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ '{source_file}' (—Å—Ö–æ–∂–µ—Å—Ç—å: {score:.2f}):\n\n{chunk_text}"
            await m.answer(response_text)
            return # –í—ã—Ö–æ–¥–∏–º, —á—Ç–æ–±—ã –Ω–µ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å –¥—Ä—É–≥–∏–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ—Å–ª–µ –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è –≤ KB
        # –ï—Å–ª–∏ –≤ KB –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –º–æ–∂–Ω–æ –ª–∏–±–æ –Ω–∏—á–µ–≥–æ –Ω–µ –¥–µ–ª–∞—Ç—å, –ª–∏–±–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å find_answer
        # else:
        #     # –ï—Å–ª–∏ find_answer –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á—Ç–æ-—Ç–æ –æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ–µ, –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–≥–æ
        #     fallback_answer = find_answer(text)
        #     if fallback_answer != "–û—Ç–≤–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω": # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ find_answer –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç "–û—Ç–≤–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω"
        #         await m.answer(fallback_answer)
        #     # –ò–ª–∏ –ø—Ä–æ—Å—Ç–æ –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∏ –∂–¥–µ–º —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
    # --- –ö–û–ù–ï–¶ –ù–û–í–û–ì–û ---

    # --- –°—Ç–∞—Ä–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–æ–≥–æ–≤–æ—Ä–∞ ---
    if "city" not in data:
        data["city"] = text
        return await m.answer("–ê—Ä–µ–Ω–¥–æ–¥–∞—Ç–µ–ª—å (–§–ò–û –ø–æ–ª–Ω–æ—Å—Ç—å—é):")
    if "landlord" not in data:
        data["landlord"] = text
        return await m.answer("–ê—Ä–µ–Ω–¥–∞—Ç–æ—Ä (–§–ò–û –ø–æ–ª–Ω–æ—Å—Ç—å—é):")
    if "tenant" not in data:
        data["tenant"] = text
        return await m.answer("–û–ø–∏—Å–∞–Ω–∏–µ –ø–æ–º–µ—â–µ–Ω–∏—è (–∞–¥—Ä–µ—Å, –ø–ª–æ—â–∞–¥—å):")
    if "property" not in data:
        data["property"] = text
        return await m.answer("–°—É–º–º–∞ –∞—Ä–µ–Ω–¥—ã –≤ –º–µ—Å—è—Ü (—Ä—É–±.):")
    if "rent" not in data:
        data["rent"] = text
        data.update({"day": "01", "month": "—è–Ω–≤–∞—Ä—è", "year": "2025",
                     "landlord_passport": "—Å–µ—Ä–∏—è 1234 ‚Ññ567890",
                     "tenant_passport":  "—Å–µ—Ä–∏—è 9876 ‚Ññ543210"})
        out_path = build(data, f"–∞—Ä–µ–Ω–¥–∞_–ò–ü_{uid}.docx")
        await m.reply_document(types.InputFile(out_path))
        data.clear()
    # --- –ö–æ–Ω–µ—Ü —Å—Ç–∞—Ä–æ–π –ª–æ–≥–∏–∫–∏ ---
    else:
        # –≠—Ç–æ—Ç else —Å—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç, –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–µ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Å–æ–∑–¥–∞–Ω–∏—è –¥–æ–≥–æ–≤–æ—Ä–∞
        # –∏ –Ω–µ –Ω–∞—à–ª–æ—Å—å –Ω–∏—á–µ–≥–æ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π (–∏–ª–∏ –µ—Å–ª–∏ –ø–æ—Ä–æ–≥ –Ω–µ –±—ã–ª –ø—Ä–æ–π–¥–µ–Ω).
        # –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å find_answer –∫–∞–∫ —Ä–µ–∑–µ—Ä–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç.
        fallback_answer = find_answer(text)
        await m.answer(fallback_answer)


if __name__ == "__main__":
    executor.start_polling(dp, skip_updates=True)

# –§—É–Ω–∫—Ü–∏—è search_in_knowledge_base —É–∂–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ –≤—ã—à–µ
