# bot.py
import pickle # –î–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
from sentence_transformers import SentenceTransformer # –î–ª—è NLP
import faiss # –î–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã
import os # –î–ª—è —Ä–∞–±–æ—Ç—ã —Å –ø—É—Ç—è–º–∏
from aiogram import Bot, Dispatcher, types
from aiogram.utils import executor
from core.config import TOKEN
from core.pdf_tool import extract_text
from core.analyzer import analyze
from core.doc_generator import build
from core.kb_search import find_answer
import pickle

logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ NLP...")
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ FAISS –∏–Ω–¥–µ–∫—Å–∞...")
index = faiss.read_index("faiss_index.bin")

logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö...")
with open("metadata.pkl", "rb") as f:
    metadata_list = pickle.load(f)

logger.info("–ú–æ–¥–µ–ª—å, –∏–Ω–¥–µ–∫—Å –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã.")
API_TOKEN = TOKEN
bot = Bot(token=API_TOKEN)
dp  = Dispatcher(bot)

user_data = {}   # {user_id: dict}

@dp.message_handler(commands="start")
async def start(m: types.Message):
    kb = types.ReplyKeyboardMarkup(resize_keyboard=True)
    kb.add("üìÑ –ê–Ω–∞–ª–∏–∑ PDF", "üìù –°–æ–∑–¥–∞—Ç—å –¥–æ–≥–æ–≤–æ—Ä", "‚ùì –ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å")
    await m.answer("üëã LegalAideIPbot ‚Äì –ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è –ò–ü", reply_markup=kb)
async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    # ... (–ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ–≥–ª–∞—Å–∏—è, —Å–æ—Å—Ç–æ—è–Ω–∏–π, –∫–æ–º–∞–Ω–¥) ...

    if state == STATE_START:
        # ... (–ø—Ä–æ–≤–µ—Ä–∫–∏ —é–º–æ—Ä–∞, –∫–Ω–æ–ø–æ–∫ –º–µ–Ω—é) ...
        # –ï—Å–ª–∏ –Ω–µ –ø–æ–¥–æ—à–ª–æ –Ω–∏ –æ–¥–Ω–æ –∏–∑ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö —É—Å–ª–æ–≤–∏–π
        user_question = update.message.text.strip()
        # –í—ã–∑–æ–≤ –ø–æ–∏—Å–∫–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
        kb_results = search_in_knowledge_base(user_question, top_k=1)

        if kb_results and kb_results[0]['score'] > 0.5: # –ü–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞
            best_match = kb_results[0]
            source_file = best_match['metadata'].get('source_file', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç')
            chunk_text = best_match['metadata'].get('original_chunk', '')[:500] + "..." # –û–±—Ä–µ–∑–∞–µ–º –¥–ª—è –≤—ã–≤–æ–¥–∞
            response_text = f"–ù–∞–π–¥–µ–Ω–æ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ '{source_file}':\n\n{chunk_text}\n\n–û—Ü–µ–Ω–∫–∞ —Å—Ö–æ–¥—Å—Ç–≤–∞: {best_match['score']:.4f}"
        else:
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å—Ç–∞—Ä—É—é –ª–æ–≥–∏–∫—É –∏–ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç
            response_text = TEXTS["unknown_message"] # –∏–ª–∏ –¥—Ä—É–≥–∞—è –ª–æ–≥–∏–∫–∞

        await update.message.reply_text(
            response_text,
            reply_markup=ReplyKeyboardMarkup(MAIN_MENU_KEYBOARD, resize_keyboard=True)
        )
        # USER_STATES[user_id] = STATE_START # –£–∂–µ –≤ START
   
@dp.message_handler(content_types=types.ContentType.DOCUMENT)
async def handle_doc(m: types.Message):
    if not m.document.file_name.lower().endswith(".pdf"):
        return await m.reply("–ü—Ä–∏—à–ª–∏—Ç–µ PDF-–¥–æ–≥–æ–≤–æ—Ä")
    with tempfile.NamedTemporaryFile(suffix=".pdf", delete=False) as tmp:
        await m.document.download(tmp.name)
        text = extract_text(tmp.name)
        os.unlink(tmp.name)
        risks = "\n".join(analyze(text))
        await m.answer(risks)

@dp.message_handler(lambda m: m.text == "üìù –°–æ–∑–¥–∞—Ç—å –¥–æ–≥–æ–≤–æ—Ä")
async def new_contract(m: types.Message):
    user_data[m.from_user.id] = {}
    await m.answer("–í–≤–µ–¥–∏—Ç–µ –≥–æ—Ä–æ–¥ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ú–æ—Å–∫–≤–∞):")

@dp.message_handler(lambda m: m.text == "‚ùì –ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å")
async def ask(m: types.Message):
    await m.answer("–ù–∞–ø–∏—à–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å:")

@dp.message_handler()
async def collect(m: types.Message):
    uid = m.from_user.id
    data = user_data.setdefault(uid, {})
    if "city" not in data:
        data["city"] = m.text
        return await m.answer("–ê—Ä–µ–Ω–¥–æ–¥–∞—Ç–µ–ª—å (–§–ò–û –ø–æ–ª–Ω–æ—Å—Ç—å—é):")
    if "landlord" not in data:
        data["landlord"] = m.text
        return await m.answer("–ê—Ä–µ–Ω–¥–∞—Ç–æ—Ä (–§–ò–û –ø–æ–ª–Ω–æ—Å—Ç—å—é):")
    if "tenant" not in data:
        data["tenant"] = m.text
        return await m.answer("–û–ø–∏—Å–∞–Ω–∏–µ –ø–æ–º–µ—â–µ–Ω–∏—è (–∞–¥—Ä–µ—Å, –ø–ª–æ—â–∞–¥—å):")
    if "property" not in data:
        data["property"] = m.text
        return await m.answer("–°—É–º–º–∞ –∞—Ä–µ–Ω–¥—ã –≤ –º–µ—Å—è—Ü (—Ä—É–±.):")
    if "rent" not in data:
        data["rent"] = m.text
        data.update({"day": "01", "month": "—è–Ω–≤–∞—Ä—è", "year": "2025",
                     "landlord_passport": "—Å–µ—Ä–∏—è 1234 ‚Ññ567890",
                     "tenant_passport":  "—Å–µ—Ä–∏—è 9876 ‚Ññ543210"})
        out_path = build(data, f"–∞—Ä–µ–Ω–¥–∞_–ò–ü_{uid}.docx")
        await m.reply_document(types.InputFile(out_path))
        data.clear()
    else:
        await m.answer(find_answer(m.text))

if __name__ == "__main__":
    executor.start_polling(dp, skip_updates=True)

def search_in_knowledge_base(query_text, top_k=1):
    """–ü–æ–∏—Å–∫ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π."""
    query_embedding = model.encode([query_text])
    faiss.normalize_L2(query_embedding.astype('float32'))
    scores, indices = index.search(query_embedding.astype('float32'), top_k)

    results = []
    for i in range(len(indices[0])):
        idx = indices[0][i]
        if 0 <= idx < len(metadata_list): # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥—Ä–∞–Ω–∏—Ü
            results.append({
                "metadata": metadata_list[idx],
                "score": scores[0][i]
            })
    return results
